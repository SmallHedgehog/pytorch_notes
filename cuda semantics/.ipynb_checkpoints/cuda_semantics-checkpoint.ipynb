{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\underline{torch.cuda} is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a \\underline{torch.cuda.device} context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda cuda:0 cuda:1\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Some examples\n",
    "cuda_s = torch.device('cuda') # default CUDA device\n",
    "cuda_0 = torch.device('cuda:0')\n",
    "cuda_1 = torch.device('cuda:1') # GPU 2\n",
    "print(cuda_d, cuda_0, cuda_1)\n",
    "\n",
    "x = torch.tensor([1., 2.], device=cuda_0)\n",
    "print(x.device)\n",
    "y = torch.tensor([1., 2.]).cuda()\n",
    "print(y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(0): # Select CUDA device 0\n",
    "    # allocates a tensor on GPU 0\n",
    "    a = torch.tensor([1., 2.], device=cuda_s)\n",
    "    print(a.device)\n",
    "    \n",
    "    # transfers a tensor from CPU to GPU 0\n",
    "    b = torch.tensor([1., 2.]).cuda()\n",
    "    print(b.device)\n",
    "    \n",
    "    # you can also use ``Tensor.to`` to transfer a tensor\n",
    "    b2 = torch.tensor([1., 2.]).to(device=cuda_s)\n",
    "    print(b2.device)\n",
    "    \n",
    "    c = a + b\n",
    "    # c.device is device(type='cuda', index=0)\n",
    "    print(c.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device-agnostic code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to the structure of PyTorch, you may need to explicitly write device-agnostic(CPU or GPU) code. \n",
    "* To determine whether the GPU should be used or not\n",
    "* To move tensors to CPU or CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--disable-cuda', action='store_true', help='Whether to disable CUDA')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.device = None\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')\n",
    "    \n",
    "# Now that we have args.device, we can use it to create a Tensor on the desired device\n",
    "x = torch.empty((10, 5), device=args.device)\n",
    "net = Network().to(device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pinned memory buffers \n",
    "* pageable memory（由操作系统API malloc()在主机上分配的，可分页、交换）\n",
    "* page-lock or pinned memory（由CUDA函数cudaHostAlloc()在主机内存上分配的，主机的操作系统将不会对这块内存进行内存分页和交换，确保该内存始终驻留在物理内存中）\n",
    "* GPU知道page-lock memory的物理地址，可以通过直接内存访问（Direct Memory Access，DMA）技术直接在主机和GPU之间复制数据，速率很快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can make the DataLoader return batches placed in pinned memory by passing pin_memory=True to its constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use nn.DataParallel instead of multiprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
