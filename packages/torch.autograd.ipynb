{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *torch.autograd* provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare *Tensor*s for which gradients should be computed with the *requires_grad=True* keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表达式：y= x^TAx\n",
    "#### 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![irVr11.jpg](https://s1.ax1x.com/2018/10/23/irVr11.jpg)](https://imgchr.com/i/irVr11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![irVHnf.jpg](https://s1.ax1x.com/2018/10/23/irVHnf.jpg)](https://imgchr.com/i/irVHnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播算梯度\n",
    "[![irVOAg.jpg](https://s1.ax1x.com/2018/10/23/irVOAg.jpg)](https://imgchr.com/i/irVOAg)\n",
    "\n",
    "[![irVXNQ.jpg](https://s1.ax1x.com/2018/10/23/irVXNQ.jpg)](https://imgchr.com/i/irVXNQ)\n",
    "\n",
    "[![irZE4J.jpg](https://s1.ax1x.com/2018/10/23/irZE4J.jpg)](https://imgchr.com/i/irZE4J)\n",
    "\n",
    "[![irZZC9.jpg](https://s1.ax1x.com/2018/10/23/irZZC9.jpg)](https://imgchr.com/i/irZZC9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导如何编码历史信息\n",
    "### 每次执行一个操作时，一个表示它的新*Function*就被实例化，它的*forward*方法被调用，并且它输出的*Tensor*的*grad_fn*被设置为这个*Function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![irnY8J.png](https://s1.ax1x.com/2018/10/23/irnY8J.png)](https://imgchr.com/i/irnY8J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the sum of gradients of given tensors w.r.t. graph leaves.\n",
    "# torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes and returns the sum of gradients of outputs w.r.t. the inputs.\n",
    "# torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True,\n",
    "# allow_unused=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally disabling gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Context-manager that disabled gradient calculation. Also functions as a decorator.\n",
    "# torch.autograd.no_grad\n",
    "\n",
    "# Example\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1], dtype=torch.float, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "print(y.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def doubler(x):\n",
    "    return x * 2\n",
    "z = doubler(x)\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context-manager that enables gradient calculation. Enables gradient calculation inside a no_grad context. This\n",
    "# has no effect outside of no_grad. Also functions as a decorator.\n",
    "# torch.autograd.enable_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Context-manager that sets gradient calculation to on or off.\n",
    "# torch.autograd.set_grad_enabled(mode)\n",
    "\n",
    "# Example\n",
    "x = torch.tensor([1], dtype=torch.float, requires_grad=True)\n",
    "is_train = False\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = x * 2\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "# torch.Tensor.backward(gradient=None, retain_graph=None, create_graph=False)\n",
    "\n",
    "# Returns a new Tensor, detached from the current graph. The result will never require gradient.\n",
    "# torch.Tensor.detach()\n",
    "\n",
    "# Detaches the Tensor from the graph that created it, making it a leaf.\n",
    "# torch.Tensor.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records operation history and defines formulas for differentiating ops.\n",
    "# torch.autograd.Function\n",
    "\n",
    "# Defines a formula for differentiating the operation. It must accept a context ctx as the first argument.\n",
    "# static backward(ctx, *grad_outputs)\n",
    "\n",
    "# Performs the operation. It must accept a context ctx as the first argument, followed by any number of arguments.\n",
    "# the context can be used to store tensors that can be then retrieved during the backward pass.\n",
    "# static forward(ctx, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
