{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Loss Functions Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.L1Loss(size_average=None, recude=None, reduction='elementwise_mean')\n",
    "l1loss = nn.L1Loss(reduction='elementwise_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<L1LossBackward>)\n",
      "tensor([[0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_x = torch.ones(10, 5, requires_grad=True)\n",
    "target = torch.zeros(10, 5)\n",
    "\n",
    "loss = l1loss(input_x, target)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(input_x.grad)\n",
    "\n",
    "print(target.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200]])\n"
     ]
    }
   ],
   "source": [
    "# Custom L1Loss\n",
    "class CustomL1Loss(nn.Module):\n",
    "    def __init__(self, reduce=True, size_average=True):\n",
    "        super(CustomL1Loss, self).__init__()\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, _input, target):\n",
    "        abs_metric = torch.abs(_input - target)\n",
    "        if self.reduce:\n",
    "            return torch.mean(abs_metric) if self.size_average else torch.sum(abs_metric)\n",
    "        else:\n",
    "            return abs_metric\n",
    "    \n",
    "l1loss = CustomL1Loss(size_average=True)\n",
    "loss = l1loss(input_x, target)\n",
    "\n",
    "print(loss)\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MseLossBackward>)\n",
      "tensor([[0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400]])\n"
     ]
    }
   ],
   "source": [
    "# nn.MSELoss(size_average=None, reduce=None, reduction='elementwise_mean')\n",
    "mseloss = nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "# input_x = torch.ones(10, 5, requires_grad=True)\n",
    "# target = torch.zeros(10, 5)\n",
    "\n",
    "loss = mseloss(input_x, target)\n",
    "\n",
    "print(loss)\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
      "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400]])\n"
     ]
    }
   ],
   "source": [
    "# Custom MSELoss\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self, reduce=True, size_average=True):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, _input, target):\n",
    "        mse_metric = torch.pow(_input - target, 2)\n",
    "        if self.reduce:\n",
    "            return torch.mean(mse_metric) if self.size_average else torch.sum(mse_metric)\n",
    "        else:\n",
    "            return mse_metric\n",
    "    \n",
    "mseloss = CustomMSELoss(size_average=True)\n",
    "\n",
    "loss = mseloss(input_x, target)\n",
    "\n",
    "print(loss)\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$loss(x, class) = -\\log(e^{x[class]} / \\sum(e^{x[j]})) = -x[class] + \\log(\\sum(e^{x[j]}))$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Math\n",
    "Math(r'loss(x, class) = -\\log(e^{x[class]} / \\sum(e^{x[j]})) = -x[class] + \\log(\\sum(e^{x[j]}))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9935, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0247,  0.0136,  0.0214, -0.0805,  0.0208],\n",
      "        [-0.0673,  0.0078,  0.0297,  0.0244,  0.0055],\n",
      "        [ 0.0292,  0.0138,  0.0183, -0.0660,  0.0047],\n",
      "        [ 0.0368,  0.0081,  0.0197,  0.0140, -0.0786],\n",
      "        [ 0.0146, -0.0920,  0.0186,  0.0171,  0.0417],\n",
      "        [-0.0719,  0.0096,  0.0280,  0.0288,  0.0054],\n",
      "        [ 0.0029,  0.0066, -0.0988,  0.0671,  0.0223],\n",
      "        [ 0.0542,  0.0024, -0.0912,  0.0072,  0.0274],\n",
      "        [ 0.0483,  0.0133,  0.0046, -0.0843,  0.0181],\n",
      "        [ 0.0559,  0.0135, -0.0870,  0.0103,  0.0073]])\n",
      "tensor(1.9793, grad_fn=<NllLossBackward>)\n",
      "tensor([3, 0, 3, 4, 1, 0, 2, 2, 3, 2])\n",
      "tensor([[ 0.0482,  0.0265,  0.0418, -0.1571,  0.0406],\n",
      "        [-0.0164,  0.0019,  0.0072,  0.0060,  0.0013],\n",
      "        [ 0.0570,  0.0269,  0.0357, -0.1289,  0.0092],\n",
      "        [ 0.0090,  0.0020,  0.0048,  0.0034, -0.0192],\n",
      "        [ 0.0071, -0.0449,  0.0091,  0.0083,  0.0203],\n",
      "        [-0.0175,  0.0023,  0.0068,  0.0070,  0.0013],\n",
      "        [ 0.0028,  0.0064, -0.0964,  0.0654,  0.0217],\n",
      "        [ 0.0529,  0.0024, -0.0890,  0.0070,  0.0267],\n",
      "        [ 0.0942,  0.0259,  0.0091, -0.1644,  0.0353],\n",
      "        [ 0.0546,  0.0132, -0.0849,  0.0100,  0.0071]])\n"
     ]
    }
   ],
   "source": [
    "# nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='elementwise_mean')\n",
    "'''This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "Parameters:\n",
    "    weight-A manual rescaling weight given to each class. If given, has to be a Tensor of size C. this is\n",
    "        particularly useful when you have an unbalanced training set.\n",
    "    ignore_index-Specifies a target value that is ignored and does not contribute to the input gradient.\n",
    "        When size_average is True, the loss is averaged over non-ignored tragets.\n",
    "'''\n",
    "CrossEntropyLoss = nn.CrossEntropyLoss(reduce=True, size_average=True)\n",
    "\n",
    "# Set RANDOM_SEED\n",
    "RANDOM_SEED = 1000\n",
    "torch.manual_seed(RANDOM_SEED) # CPU\n",
    "torch.cuda.manual_seed(RANDOM_SEED) # GPU\n",
    "np.random.seed(RANDOM_SEED) # Numpy\n",
    "random.seed(RANDOM_SEED) # Random\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "input_x = torch.randn(10, 5, requires_grad=True)\n",
    "target = torch.empty(10, dtype=torch.long).random_(5)\n",
    "\n",
    "loss = CrossEntropyLoss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# CrossEntropyLoss with weight\n",
    "weights = torch.FloatTensor([0.1, 0.2, 0.4, 0.8, 0.1])\n",
    "CrossEntropyLoss = nn.CrossEntropyLoss(weight=weights, reduce=True, size_average=True)\n",
    "loss = CrossEntropyLoss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(target)\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9935, grad_fn=<MeanBackward1>)\n",
      "tensor([[ 0.0247,  0.0136,  0.0214, -0.0805,  0.0208],\n",
      "        [-0.0673,  0.0078,  0.0297,  0.0244,  0.0055],\n",
      "        [ 0.0292,  0.0138,  0.0183, -0.0660,  0.0047],\n",
      "        [ 0.0368,  0.0081,  0.0197,  0.0140, -0.0786],\n",
      "        [ 0.0146, -0.0920,  0.0186,  0.0171,  0.0417],\n",
      "        [-0.0719,  0.0096,  0.0280,  0.0288,  0.0054],\n",
      "        [ 0.0029,  0.0066, -0.0988,  0.0671,  0.0223],\n",
      "        [ 0.0542,  0.0024, -0.0912,  0.0072,  0.0274],\n",
      "        [ 0.0483,  0.0133,  0.0046, -0.0843,  0.0181],\n",
      "        [ 0.0559,  0.0135, -0.0870,  0.0103,  0.0073]])\n",
      "tensor(1.9793, grad_fn=<SumBackward0>)\n",
      "tensor([3, 0, 3, 4, 1, 0, 2, 2, 3, 2])\n",
      "tensor([[ 0.0482,  0.0265,  0.0418, -0.1571,  0.0406],\n",
      "        [-0.0164,  0.0019,  0.0072,  0.0060,  0.0013],\n",
      "        [ 0.0570,  0.0269,  0.0357, -0.1289,  0.0092],\n",
      "        [ 0.0090,  0.0020,  0.0048,  0.0034, -0.0192],\n",
      "        [ 0.0071, -0.0449,  0.0091,  0.0083,  0.0203],\n",
      "        [-0.0175,  0.0023,  0.0068,  0.0070,  0.0013],\n",
      "        [ 0.0028,  0.0064, -0.0964,  0.0654,  0.0217],\n",
      "        [ 0.0529,  0.0024, -0.0890,  0.0070,  0.0267],\n",
      "        [ 0.0942,  0.0259,  0.0091, -0.1644,  0.0353],\n",
      "        [ 0.0546,  0.0132, -0.0849,  0.0100,  0.0071]])\n"
     ]
    }
   ],
   "source": [
    "# Custom CrossEntropyLoss\n",
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduce=True, size_average=True):\n",
    "        super(CustomCrossEntropyLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, input_x, target):\n",
    "        '''Parameters:\n",
    "            input_x: (minibatch, C)\n",
    "            target: (minibatch) where each value is 0 <= target[i] <= C-1\n",
    "        '''\n",
    "        row_idxs = torch.arange(input_x.size(0))\n",
    "        log_sum = torch.log(torch.sum(torch.exp(input_x), 1))\n",
    "        \n",
    "        cross_entropy_metric = -1 * input_x[row_idxs, target] + log_sum\n",
    "        if self.weight is not None:\n",
    "            # TODO: assert\n",
    "            cross_entropy_metric = self.weight[target] * cross_entropy_metric\n",
    "        if self.reduce:\n",
    "            if self.size_average:\n",
    "                if self.weight is None:\n",
    "                    return torch.mean(cross_entropy_metric)\n",
    "                return torch.sum((1.0 / self.weight[target].sum()) * cross_entropy_metric)\n",
    "            else:\n",
    "                return torch.sum(cross_entropy_metric)\n",
    "        else:\n",
    "            return cross_entropy_metric\n",
    "        \n",
    "CrossEntropyLoss = CustomCrossEntropyLoss(size_average=True)\n",
    "loss = CrossEntropyLoss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "weights = torch.FloatTensor([0.1, 0.2, 0.4, 0.8, 0.1])\n",
    "CrossEntropyLoss = CustomCrossEntropyLoss(weight=weights, reduce=True, size_average=True)\n",
    "loss = CrossEntropyLoss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(target)\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input given through a forward call is expected to contain log-probabilities of each class.\\n    Parameters: see nn.CrossEntropyLoss's parameters for more details.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='elementwise_mean')\n",
    "'''The input given through a forward call is expected to contain log-probabilities of each class.\n",
    "    Parameters: see nn.CrossEntropyLoss's parameters for more details.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![image.png](https://s1.ax1x.com/2018/10/21/iBdNTJ.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9935, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0247,  0.0136,  0.0214, -0.0805,  0.0208],\n",
      "        [-0.0673,  0.0078,  0.0297,  0.0244,  0.0055],\n",
      "        [ 0.0292,  0.0138,  0.0183, -0.0660,  0.0047],\n",
      "        [ 0.0368,  0.0081,  0.0197,  0.0140, -0.0786],\n",
      "        [ 0.0146, -0.0920,  0.0186,  0.0171,  0.0417],\n",
      "        [-0.0719,  0.0096,  0.0280,  0.0288,  0.0054],\n",
      "        [ 0.0029,  0.0066, -0.0988,  0.0671,  0.0223],\n",
      "        [ 0.0542,  0.0024, -0.0912,  0.0072,  0.0274],\n",
      "        [ 0.0483,  0.0133,  0.0046, -0.0843,  0.0181],\n",
      "        [ 0.0559,  0.0135, -0.0870,  0.0103,  0.0073]])\n",
      "tensor(1.9793, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0482,  0.0265,  0.0418, -0.1571,  0.0406],\n",
      "        [-0.0164,  0.0019,  0.0072,  0.0060,  0.0013],\n",
      "        [ 0.0570,  0.0269,  0.0357, -0.1289,  0.0092],\n",
      "        [ 0.0090,  0.0020,  0.0048,  0.0034, -0.0192],\n",
      "        [ 0.0071, -0.0449,  0.0091,  0.0083,  0.0203],\n",
      "        [-0.0175,  0.0023,  0.0068,  0.0070,  0.0013],\n",
      "        [ 0.0028,  0.0064, -0.0964,  0.0654,  0.0217],\n",
      "        [ 0.0529,  0.0024, -0.0890,  0.0070,  0.0267],\n",
      "        [ 0.0942,  0.0259,  0.0091, -0.1644,  0.0353],\n",
      "        [ 0.0546,  0.0132, -0.0849,  0.0100,  0.0071]])\n"
     ]
    }
   ],
   "source": [
    "# nllloss = nn.NLLLoss(reduce=True, size_average=True)\n",
    "nllloss = nn.NLLLoss(reduction='elementwise_mean')\n",
    "\n",
    "# Need to contain log-probabilities of each of class.\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "loss = nllloss(log_softmax(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# nn.NLLLoss with weiht\n",
    "# nllloss = nn.NLLLoss(weight=weights, reduce=True, size_average=True)\n",
    "nllloss = nn.NLLLoss(weight=weights, reduction='elementwise_mean')\n",
    "loss = nllloss(log_softmax(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9935, grad_fn=<MeanBackward1>)\n",
      "tensor([[ 0.0247,  0.0136,  0.0214, -0.0805,  0.0208],\n",
      "        [-0.0673,  0.0078,  0.0297,  0.0244,  0.0055],\n",
      "        [ 0.0292,  0.0138,  0.0183, -0.0660,  0.0047],\n",
      "        [ 0.0368,  0.0081,  0.0197,  0.0140, -0.0786],\n",
      "        [ 0.0146, -0.0920,  0.0186,  0.0171,  0.0417],\n",
      "        [-0.0719,  0.0096,  0.0280,  0.0288,  0.0054],\n",
      "        [ 0.0029,  0.0066, -0.0988,  0.0671,  0.0223],\n",
      "        [ 0.0542,  0.0024, -0.0912,  0.0072,  0.0274],\n",
      "        [ 0.0483,  0.0133,  0.0046, -0.0843,  0.0181],\n",
      "        [ 0.0559,  0.0135, -0.0870,  0.0103,  0.0073]])\n",
      "tensor(1.9793, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.0482,  0.0265,  0.0418, -0.1571,  0.0406],\n",
      "        [-0.0164,  0.0019,  0.0072,  0.0060,  0.0013],\n",
      "        [ 0.0570,  0.0269,  0.0357, -0.1289,  0.0092],\n",
      "        [ 0.0090,  0.0020,  0.0048,  0.0034, -0.0192],\n",
      "        [ 0.0071, -0.0449,  0.0091,  0.0083,  0.0203],\n",
      "        [-0.0175,  0.0023,  0.0068,  0.0070,  0.0013],\n",
      "        [ 0.0028,  0.0064, -0.0964,  0.0654,  0.0217],\n",
      "        [ 0.0529,  0.0024, -0.0890,  0.0070,  0.0267],\n",
      "        [ 0.0942,  0.0259,  0.0091, -0.1644,  0.0353],\n",
      "        [ 0.0546,  0.0132, -0.0849,  0.0100,  0.0071]])\n"
     ]
    }
   ],
   "source": [
    "# CustomNLLLoss\n",
    "class CustomNLLLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduce=True, size_average=True):\n",
    "        super(CustomNLLLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, input_x, target):\n",
    "        '''Parameters:\n",
    "            input_x: (minibatch, C) where each value is log-probabilities\n",
    "            target: (minibatch) where each value is 0 <= target[i] <= C-1\n",
    "        '''\n",
    "        row_idxs = torch.arange(input_x.size(0))\n",
    "        nllloss_metric = -input_x[row_idxs, target]\n",
    "        if self.weight is not None:\n",
    "            # TODO: assert\n",
    "            nllloss_metric = self.weight[target] * nllloss_metric\n",
    "            \n",
    "        if self.reduce:\n",
    "            if self.size_average:\n",
    "                if self.weight is not None:\n",
    "                    return torch.sum((1.0 / self.weight[target].sum()) * nllloss_metric)\n",
    "                else:\n",
    "                    return torch.mean(nllloss_metric)\n",
    "            else:\n",
    "                return torch.sum(nllloss_metric)\n",
    "        else:\n",
    "            return nllloss_metric\n",
    "        \n",
    "# CustomNLLLoss without weight\n",
    "nllloss = CustomNLLLoss(reduce=True, size_average=True)\n",
    "loss = nllloss(log_softmax(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# CustomNLLLoss with weight\n",
    "nllloss = CustomNLLLoss(weight=weights, reduce=True, size_average=True)\n",
    "loss = nllloss(log_softmax(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.BCELoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates a criterion that measures the Binary Cross Entropy between the target and the output.\\n    Parameters: to see nn.CrossEntropyLoss\\'s parameters for more details.\\n        weight-A manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size \"nbatch\". \\nNote that the targets y should be numbers between 0 and 1.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='elementwise_mean')\n",
    "'''Creates a criterion that measures the Binary Cross Entropy between the target and the output.\n",
    "    Parameters: to see nn.CrossEntropyLoss's parameters for more details.\n",
    "        weight-A manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size \"nbatch\". \n",
    "Note that the targets y should be numbers between 0 and 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![iBs861.png](https://s1.ax1x.com/2018/10/21/iBs861.png)](https://imgchr.com/i/iBs861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9453, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor([ 0.0262, -0.0762, -0.0492,  0.0358,  0.0891, -0.0288,  0.0289, -0.0656,\n",
      "        -0.0840,  0.0549])\n",
      "tensor(0.4822, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor([ 0.0105, -0.0305, -0.0197,  0.0215,  0.0534, -0.0173,  0.0173, -0.0394,\n",
      "        -0.0336,  0.0330])\n"
     ]
    }
   ],
   "source": [
    "# Need to get a probability value \n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# nn.BCELoss without weight\n",
    "bceloss = nn.BCELoss(weight=None, reduction='elementwise_mean')\n",
    "\n",
    "input_x = torch.randn(10, requires_grad=True)\n",
    "target = torch.empty(10).random_(2)\n",
    "\n",
    "loss = bceloss(sigmoid(input_x), target)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# nn.BCELoss with weight\n",
    "weights = torch.FloatTensor([0.4, 0.6]) # weight of each class\n",
    "weights = weights[torch.empty(10, dtype=torch.long).random_(2)]\n",
    "\n",
    "bceloss = nn.BCELoss(weight=weights, reduction='elementwise_mean')\n",
    "loss = bceloss(sigmoid(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9453, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0262, -0.0762, -0.0492,  0.0358,  0.0891, -0.0288,  0.0289, -0.0656,\n",
      "        -0.0840,  0.0549])\n",
      "tensor(0.4822, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0105, -0.0305, -0.0197,  0.0215,  0.0534, -0.0173,  0.0173, -0.0394,\n",
      "        -0.0336,  0.0330])\n"
     ]
    }
   ],
   "source": [
    "# CustomBCELoss\n",
    "class CustomBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduce=True, size_average=True):\n",
    "        super(CustomBCELoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, input_x, target):\n",
    "        '''Parameters:\n",
    "            input_x = sigmoid(input_x) with shape (N, )\n",
    "            target with shape (N, )\n",
    "        '''\n",
    "        bce_metric = target * torch.log(input_x) + (1 - target) * torch.log(1 - input_x)\n",
    "        bce_metric = -1.0 * bce_metric\n",
    "        if self.weight is not None:\n",
    "            bce_metric = self.weight * bce_metric\n",
    "            \n",
    "        if self.reduce:\n",
    "            return torch.mean(bce_metric) if self.size_average else torch.sum(bce_metric)\n",
    "        else:\n",
    "            return bce_metric\n",
    "        \n",
    "# CustomBCELoss without weight\n",
    "bceloss = CustomBCELoss(weight=None, reduce=True, size_average=True)\n",
    "loss = bceloss(sigmoid(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# CustomBCELoss with weight\n",
    "bceloss = CustomBCELoss(weight=weights, reduce=True, size_average=True)\n",
    "loss = bceloss(sigmoid(input_x), target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.BCEWithLogitsLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This loss conmbines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable\\nthan using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage\\nof the log-sum-exp trick for numerical stability.\\n    Parameters:\\n        pos_weight-It's possible to trade off recall and precision by adding weights to positive examples.\\n    must be a vector with length equal to the number of classes.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='elementwise_mean', pos_weight=None)\n",
    "'''This loss conmbines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable\n",
    "than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage\n",
    "of the log-sum-exp trick for numerical stability.\n",
    "    Parameters:\n",
    "        pos_weight-It's possible to trade off recall and precision by adding weights to positive examples.\n",
    "    must be a vector with length equal to the number of classes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![iBgYgH.png](https://s1.ax1x.com/2018/10/21/iBgYgH.png)](https://imgchr.com/i/iBgYgH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9453, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0262, -0.0762, -0.0492,  0.0358,  0.0891, -0.0288,  0.0289, -0.0656,\n",
      "        -0.0840,  0.0549])\n",
      "tensor(0.4822, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0105, -0.0305, -0.0197,  0.0215,  0.0534, -0.0173,  0.0173, -0.0394,\n",
      "        -0.0336,  0.0330])\n",
      "tensor([0.7000, 0.3000, 0.7000, 0.7000, 0.3000, 0.3000, 0.7000, 0.3000, 0.3000,\n",
      "        0.7000])\n",
      "tensor(0.5976, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0262, -0.0229, -0.0345,  0.0358,  0.0891, -0.0086,  0.0289, -0.0197,\n",
      "        -0.0252,  0.0549])\n"
     ]
    }
   ],
   "source": [
    "# nn.BCEWithLogitsLoss without weight\n",
    "bcewithlogitsloss = nn.BCEWithLogitsLoss(weight=None, reduction='elementwise_mean')\n",
    "loss = bcewithlogitsloss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# nn.BCEWithLogitsLoss with weight\n",
    "bcewithlogitsloss = nn.BCEWithLogitsLoss(weight=weights, reduction='elementwise_mean')\n",
    "loss = bcewithlogitsloss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# nn.BCEWithLogitsLoss with pos_weight\n",
    "pos_weights = torch.FloatTensor([0.7, 0.3])\n",
    "pos_weights = pos_weights[torch.empty(10, dtype=torch.long).random_(2)]\n",
    "print(pos_weights)\n",
    "bcewithlogitsloss = nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction='elementwise_mean')\n",
    "loss = bcewithlogitsloss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9453, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0262, -0.0762, -0.0492,  0.0358,  0.0891, -0.0288,  0.0289, -0.0656,\n",
      "        -0.0840,  0.0549])\n",
      "tensor(0.4822, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0105, -0.0305, -0.0197,  0.0215,  0.0534, -0.0173,  0.0173, -0.0394,\n",
      "        -0.0336,  0.0330])\n",
      "tensor([0.3000, 0.7000, 0.7000, 0.3000, 0.3000, 0.3000, 0.7000, 0.3000, 0.7000,\n",
      "        0.3000])\n",
      "tensor(0.7284, grad_fn=<MeanBackward1>)\n",
      "tensor([ 0.0262, -0.0534, -0.0345,  0.0358,  0.0891, -0.0086,  0.0289, -0.0197,\n",
      "        -0.0588,  0.0549])\n"
     ]
    }
   ],
   "source": [
    "# CustomBCEWithLogitsLoss\n",
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduce=True, size_average=True, pos_weight=None):\n",
    "        super(CustomBCEWithLogitsLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        self.pos_weight = pos_weight\n",
    "        \n",
    "    def forward(self, input_x, target):\n",
    "        if self.pos_weight is not None:\n",
    "            bce_metric = (target - 1) * input_x + (target * (1 - self.pos_weight) - 1) * torch.log((1 + torch.exp(-input_x)))\n",
    "        else:\n",
    "            bce_metric = (target - 1) * input_x - torch.log((1 + torch.exp(-input_x)))\n",
    "        \n",
    "        bce_metric = -1.0 * bce_metric\n",
    "        if self.weight is not None:\n",
    "            bce_metric = self.weight * bce_metric\n",
    "            \n",
    "        if self.reduce:\n",
    "            return torch.mean(bce_metric) if self.size_average else torch.sum(bce_metric)\n",
    "        else:\n",
    "            return bce_metric\n",
    "        \n",
    "# CustomBCEWithLogitsLoss without weight\n",
    "bcewithlogitsloss = CustomBCEWithLogitsLoss(weight=None, reduce=True, size_average=True)\n",
    "loss = bcewithlogitsloss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# CustomBCEWithLogitsLoss with weight\n",
    "bcewithlogitsloss = CustomBCEWithLogitsLoss(weight=weights, reduce=True, size_average=True)\n",
    "loss = bcewithlogitsloss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)\n",
    "\n",
    "# CustomBCEWithLogitsLoss with pos_weight\n",
    "pos_weights = torch.FloatTensor([0.7, 0.3])\n",
    "pos_weights = pos_weights[torch.empty(10, dtype=torch.long).random_(2)]\n",
    "print(pos_weights)\n",
    "bcewithlogitsloss = CustomBCEWithLogitsLoss(pos_weight=pos_weights, reduce=True, size_average=True)\n",
    "loss = bcewithlogitsloss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.SmoothL1Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term\\notherwise. It is less sensitive to outliers than the MSELoss and in some case prevents exploding graients.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.SmoothL1Loss(size_average=None, reduce=None, reduction='elementwise_mean')\n",
    "'''Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term\n",
    "otherwise. It is less sensitive to outliers than the MSELoss and in some case prevents exploding graients.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![iB7KAJ.png](https://s1.ax1x.com/2018/10/21/iB7KAJ.png)](https://imgchr.com/i/iB7KAJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7673, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor([[ 0.0098, -0.0232,  0.0016, -0.0079],\n",
      "        [-0.0145, -0.0250,  0.0187, -0.0250],\n",
      "        [ 0.0250,  0.0250, -0.0153,  0.0250],\n",
      "        [ 0.0250, -0.0250,  0.0211, -0.0250],\n",
      "        [-0.0250,  0.0250,  0.0033,  0.0121],\n",
      "        [-0.0250,  0.0184,  0.0250,  0.0250],\n",
      "        [-0.0039,  0.0086,  0.0250,  0.0135],\n",
      "        [-0.0250,  0.0131, -0.0149,  0.0250],\n",
      "        [-0.0247, -0.0250, -0.0250,  0.0054],\n",
      "        [ 0.0201,  0.0092,  0.0147, -0.0250]])\n"
     ]
    }
   ],
   "source": [
    "# Generate input and target\n",
    "input_x = torch.randn(10, 4, requires_grad=True)\n",
    "target = torch.randn(10, 4)\n",
    "\n",
    "# nn.SmoothL1Loss\n",
    "smoothl1loss = nn.SmoothL1Loss(reduction='elementwise_mean')\n",
    "loss = smoothl1loss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7596, grad_fn=<DivBackward0>)\n",
      "tensor([[ 0.1000, -0.1000,  0.1000, -0.1000],\n",
      "        [-0.1000, -0.1000,  0.1000, -0.1000],\n",
      "        [ 0.1000,  0.1000, -0.1000,  0.1000],\n",
      "        [ 0.1000, -0.1000,  0.1000, -0.1000],\n",
      "        [-0.1000,  0.1000,  0.1000,  0.1000],\n",
      "        [-0.1000,  0.1000,  0.1000,  0.1000],\n",
      "        [-0.1000,  0.1000,  0.1000,  0.1000],\n",
      "        [-0.1000,  0.1000, -0.1000,  0.1000],\n",
      "        [-0.1000, -0.1000, -0.1000,  0.1000],\n",
      "        [ 0.1000,  0.1000,  0.1000, -0.1000]])\n"
     ]
    }
   ],
   "source": [
    "# CustomSmoothL1Loss\n",
    "class CustomSmoothL1Loss(nn.Module):\n",
    "    def __init__(self, reduce=True, size_average=True):\n",
    "        super(CustomSmoothL1Loss, self).__init__()\n",
    "        self.reduce = reduce\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, input_x, target):\n",
    "        lt_idxs = torch.sum(torch.abs(input_x - target), dim=1) < 1\n",
    "        gq_idxs = lt_idxs == 0\n",
    "        \n",
    "        l1loss_metric1 = 0.5 * torch.pow(input_x[lt_idxs] - target[lt_idxs], 2)\n",
    "        l1loss_metric2 = torch.abs(input_x[gq_idxs] - target[gq_idxs]) - 0.5\n",
    "        \n",
    "        l1loss_metric = torch.sum(l1loss_metric1) + torch.sum(l1loss_metric2)\n",
    "        if self.reduce:\n",
    "            return (l1loss_metric / input_x.size(0)) if self.size_average else torch.sum(l1loss_metric)\n",
    "        else:\n",
    "            return l1loss_metric\n",
    "        \n",
    "# CustomSmoothL1Loss\n",
    "smoothl1loss = CustomSmoothL1Loss(reduce=True, size_average=True)\n",
    "loss = smoothl1loss(input_x, target)\n",
    "print(loss)\n",
    "\n",
    "input_x.grad = None\n",
    "loss.backward()\n",
    "print(input_x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
